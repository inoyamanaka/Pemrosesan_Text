{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rdjyiZLewlb6R2t1SltMF32qu-VN-RHS",
      "authorship_tag": "ABX9TyPyyaQ7U1yOKz1Kw7p+27vK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inoyamanaka/Pemrosesan_Text/blob/main/Tf-IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILq4s8GAtk3p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tweepy\n",
        "import csv\n",
        "import re\n",
        "from google.colab import files\n",
        "!pip install git+https://github.com/tweepy/tweepy.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')\n",
        "\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "9uTzu4kAUslH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"wjfC7sqEsgkkOtAh3QUDKTTGJ\"\n",
        "api_secret = \"TAwEmJ1MhYMgKwJBngh4ZpXnWdNFbdj778fiq6UkLFuLZ5ExKQ\"\n",
        "access_token = \"1198104989102567424-noL01z6l2PvZvtfgwhrebESA3dmwN9\"\n",
        "access_secret = \"gFTvVQoIO4J7gZ2yqVf5It6GaZmbtCz61FQphSfVJIHTJ\"\n",
        "\n",
        "auth = tweepy.OAuthHandler(api_key,api_secret)\n",
        "auth.set_access_token(access_token,access_secret)\n",
        "\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "api.verify_credentials()"
      ],
      "metadata": {
        "id": "nQ1mJHEhxUsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES CRAWLING DATA DARI TWITTER"
      ],
      "metadata": {
        "id": "xv7YyNeUVTtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets, \"Kkb Papua\", count=50).items(50):\n",
        "    user = tweet.user.screen_name\n",
        "    tweet_id = tweet.id\n",
        "    list_output = (tweet.text)\n",
        "    output.append(list_output)"
      ],
      "metadata": {
        "id": "d0brIJIaxl0h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pd.read_csv('output (10).csv', names=['Text'])\n",
        "outputs"
      ],
      "metadata": {
        "id": "nkHuFo3fopaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(outputs,columns =['Text'])\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "K77CPra9tsOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES CASE FOLDING"
      ],
      "metadata": {
        "id": "ZHMBZSUgVX_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(str_text):\n",
        "  # 1. Casefolding - mengubah ke case yang sama semua (misal huruf kecil semua)\n",
        "  str_text = str_text.lower()\n",
        "  return str_text\n",
        "\n",
        "df['Hasil Case Folding'] = df['Text'].apply(lambda x: preprocess(x))"
      ],
      "metadata": {
        "id": "PH8lMVKTyoq_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES TOKENIZE"
      ],
      "metadata": {
        "id": "nGqBpxdvVnIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(str_text):\n",
        "  # 1. Hapus mention (@..) - menggunakan regular expression\n",
        "  str_text = re.sub(r'@[\\w]+',' ',str_text)\n",
        "  # 2. Hapus hashtag (#..) - menggunakan regular expression\n",
        "  str_text = re.sub(r'#[\\w]+',' ',str_text)\n",
        "  # Langkah 2 dan 3 bisa digabung dengan kode re: r'[@|#][\\w]+'\n",
        "  # 3. Hapus URL\n",
        "  str_text = re.sub(r'http\\S+',' ',str_text)\n",
        "  # 4. Hapus non-printable chars, tanda baca, angka (alias hapus selain huruf)\n",
        "  str_text = re.sub(r'[^a-zA-Z]+',' ',str_text)\n",
        "\n",
        "  # Tokenize kalimat\n",
        "  str_text = re.split(\"\\s\", str_text)\n",
        "  return str_text\n",
        "\n",
        "df['Hasil Tokenize'] = df['Hasil Case Folding'].apply(lambda x: preprocess(x))\n",
        "df"
      ],
      "metadata": {
        "id": "Hd-lwaRozyv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES FILTERING KATA"
      ],
      "metadata": {
        "id": "zi-9HqP8VLH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering words\n",
        "ban_words = ['rt','dan', 'dari' ,'serta' ,'melainkan' ,'padahal' ,'sedangkan' ,'atau', 'tetapi',\n",
        "             'jika','jikalau','bahwa','sehingga','sudah', 'yang','padahal', 'sedangkan', 'sambil',\n",
        "             'supaya', 'agar', 'untuk','yg','aja','ini','daripada','ke','di','n','dengan','jadi',\n",
        "             'd','adanya','saat','bahkan','sangat','dah','gw','doang','aja','ini','hanya','dalam',\n",
        "             'tak','adalah','adanya','termasu','karena','mengapa', 'tidak', 'ada','b','t','g','p',\n",
        "              'hrus', 'bisa','dob','bukan','dalam','malah','akibat','pem','md','lagi','tapi','sono',\n",
        "             'sana','drun','bacot', 'p','pua','terbaikdi','drun','gorok','oap','pua', 'mbak', 'kampung',\n",
        "             'tumb', 'lkan','ny','usah','ntai','roris','ring','artinya','juga','ara','dong','yaa','cuma',\n",
        "             'gore','akan','maybrat','oleh','sugapa','k','hari','akh','mengut','sebenarnya','pa',\n",
        "             'begitu','kira','kurang','lebihnya','bro','jelas','malah','aneh','anies','saatnya',\n",
        "             'terima','kasih','ammi','gat','hendrik','']\n",
        "\n",
        "\n",
        "def filter2(str_text):\n",
        "  # Hapus kata ban words\n",
        "  for i in str_text:\n",
        "    if i in ban_words:\n",
        "      str_text.remove(i)\n",
        "\n",
        "  return str_text\n",
        "    \n",
        "# df['Hasil Filtering'] = df['Hasil Tokenize'].apply(lambda x: filter1(x))\n",
        "df['Hasil Filtering'] = df['Hasil Tokenize'].apply(lambda x: filter2(x))\n",
        "df[['Hasil Filtering']]"
      ],
      "metadata": {
        "id": "WQ701JCut6DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES STEMMING"
      ],
      "metadata": {
        "id": "dj0EffXrVEcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING\n",
        "# kamus normalisasi\n",
        "norm = {'pembentukan':'bentuk','pendidikan':'didik','bukanlah':'bukan','pedalaman':'dalam','terdepan':'depan',\n",
        "        'kesejahteraan':'sejahtera','terwujud':'wujud','pemekaran':'mekar','dianggap':'anggap','lihatlah':'lihat',\n",
        "        'dihentikan':'henti','menghambat':'hambat','pembangunan':'bangun','berdemo':'demo','menolak':'tolak',\n",
        "        'pendukung':'dukung','menebarkan':'tebar','mengumumkan':'umum','nguasain':'kuasa','terbaik':'baik','pekerja':'kerja',\n",
        "        'tersangka':'sangka','merajalela':'rajalela','melakukan':'laku','menghambat':'hambat','kemajuan':'maju','memasok':'pasok',\n",
        "        'mengungsi':'ungsi','penyerangan':'serang','mengakibatkan':'akibat','ditembak':'tembak','berhari':'hari','penanganan':'angan',\n",
        "        'menggangap':'anggap','menghalangi':'alang','tersangka':'sangka','terduga':'duga','membangun':'bangun','menganggap':'anggap',\n",
        "        'mendukung':'dukung','memajukan':'maju','serangan':'serang','pendekatan':'dekat'\n",
        "        }"
      ],
      "metadata": {
        "id": "mxMpsC9c8Y1R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(str_text):\n",
        "  replacer = norm.get  \n",
        "  return [replacer(n, n) for n in str_text]\n",
        "\n",
        "df['Hasil Stemming'] = df['Hasil Filtering'].apply(lambda x: preprocess(x))\n",
        "df[['Hasil Stemming']]"
      ],
      "metadata": {
        "id": "7uwT7kSGAd_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES MENYIMPAN KEDALAM EXCEL"
      ],
      "metadata": {
        "id": "cbZia05-WS2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Text','Hasil Case Folding']].to_excel(\"compact.xlsx\")"
      ],
      "metadata": {
        "id": "p7gDinmDL6DT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Hasil Case Folding','Hasil Tokenize']].to_excel(\"compact1.xlsx\")\n",
        "df[['Hasil Tokenize','Hasil Filtering']].to_excel(\"compact2.xlsx\")\n",
        "df[['Hasil Filtering','Hasil Stemming']].to_excel(\"compact3.xlsx\")"
      ],
      "metadata": {
        "id": "V0GixK_-OTuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
        "\n",
        "## PROSES TF-IDF SCRATCH"
      ],
      "metadata": {
        "id": "FpdxoTBPWYnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mylist = []\n",
        "def gabung(str_text):\n",
        "  Mylist.extend(str_text)\n",
        "  return Mylist\n",
        "\n",
        "def my_function(x):\n",
        "  return list(dict.fromkeys(x))\n",
        "    \n",
        "baru = df['Hasil Stemming'].apply(lambda x: gabung(x))\n",
        "data_baru = baru[0]"
      ],
      "metadata": {
        "id": "qLvC6EeCcSzk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_baru = my_function(data_baru)\n",
        "data_baru.remove('')\n",
        "word_set = data_baru"
      ],
      "metadata": {
        "id": "VcIhQwv9fJ2v"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_set"
      ],
      "metadata": {
        "id": "b6H3RITVeuzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MENGHITUNG JUMLAH KATA YANG KEMBAR PADA MASING-MASING TEXT\n",
        "def cnt(str_text):\n",
        "  word_count = {}\n",
        "  jumlah = []\n",
        "  for word in (word_set):\n",
        "      word_count[word] = 0\n",
        "      for sent in str_text:\n",
        "          if word in sent:\n",
        "              word_count[word] += 1\n",
        "\n",
        "  # MENGEBALIKAN JUMLAH KATA DALAM BENTUK DICRIONARY\n",
        "  for key in word_count:\n",
        "    jumlah.append(word_count[key])\n",
        "  return jumlah\n",
        "\n",
        "\n",
        "# MEMBUAT KOLOM STEMMING MENJADI DALAM BENTUK LIST\n",
        "def gabung2(str_text):\n",
        "  Mylist2 = []\n",
        "  Mylist2.extend(str_text)\n",
        "  return Mylist2\n",
        "    \n",
        "# MEMBUAT TABEL BARU DENGAN ATRIBUT KATA\n",
        "df_count = pd.DataFrame(word_set, columns =['Kata'])\n",
        "\n",
        "# MEMANGGIL FUNGSI GABUNG2 \n",
        "we_n = df['Hasil Stemming'].apply(lambda x: gabung2(x))\n",
        "\n",
        "# MELAKUKAN LOOPING SEMBARI MENAMBAHKAN KOLOM PADA TABEL\n",
        "for i in range (1,51):\n",
        "  df_count[f'D{i}'] = cnt(we_n[i-1])\n",
        "\n",
        "df_count\n"
      ],
      "metadata": {
        "id": "cWJngKChzBrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "SOJtW2zVdXUw",
        "outputId": "10c9fc78-ed35-4b26-9eda-5277fb983389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF Vectorize\n"
      ],
      "metadata": {
        "id": "-oFW7ONLddjL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_count.to_excel(\"final.xlsx\")"
      ],
      "metadata": {
        "id": "Fy9SaltUUnQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}